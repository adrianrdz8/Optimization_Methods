{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conjugate_Gradient_Method.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhGrHslT+5kT9IoxnZF+Vy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "icxdV-qUT5kl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.optimize as sopt\n",
        "import sklearn.datasets\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beta Methods"
      ],
      "metadata": {
        "id": "rGA558ZeUMLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fletcher-reeves conjugate gradient update\n",
        "def beta_fr(gradient, old_gradient):\n",
        "  beta = (gradient@gradient.T)/(old_gradient@old_gradient.T)\n",
        "  return beta\n",
        "\n",
        "#Polak-Riberie conjugate gradient update\n",
        "def beta_pr(gradient, old_gradient):\n",
        "  y_hat = gradient - old_gradient\n",
        "  beta = (gradient@y_hat.T)/(old_gradient@old_gradient.T)\n",
        "  return beta"
      ],
      "metadata": {
        "id": "AysEe8QyUFGg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Line Search Methods"
      ],
      "metadata": {
        "id": "6USM24TpUaLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exact Line Search (Bisection) method\n",
        "def bisection(func, x, direction, eps=1e-3):\n",
        "  a = 0\n",
        "  b = 1\n",
        "  c = (b+a)/2\n",
        "  error = abs(b-a)\n",
        "  count = 0\n",
        "  def f(t):\n",
        "    value, grad = func(x + (t*direction), 1)\n",
        "    return abs(grad)\n",
        "\n",
        "  while error > eps:\n",
        "    if f(a) < f(b):\n",
        "      b = c \n",
        "      error = abs(b-a)\n",
        "    else:\n",
        "      a = c\n",
        "      error = abs(b-a)\n",
        "    c_old = c\n",
        "    c = (b+a)/2\n",
        "    dif = abs(c - c_old)\n",
        "    if dif < eps:\n",
        "      return c\n",
        "    count += 1\n",
        "  return c"
      ],
      "metadata": {
        "id": "qOONA0ZpUdB4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Backtracking line search\n",
        "def backtracking_cg(func, x, direction, iterations = 10, eps = 1e-5 ):\n",
        "    alpha = 0.25\n",
        "    beta = 0.5\n",
        "    t = 1\n",
        "    count = 0\n",
        "    n = len(direction)\n",
        "    approx = -float('inf')\n",
        "    while func(x + t*direction, order = 0) >= approx and count <= iterations:\n",
        "        val,grad = func(x, order = 1)\n",
        "        approx = val + alpha*t*grad*direction.T\n",
        "        t = t*beta\n",
        "        count += 1\n",
        "    return t "
      ],
      "metadata": {
        "id": "ERsiA3KKUcWv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conjugate Gradient Algorithm"
      ],
      "metadata": {
        "id": "mTe-1zKrUNhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cg( func, initial_x, beta_method = beta_fr, eps=1e-5, maximum_iterations=65536, linesearch=None, *linesearch_args  ):\n",
        "#def cg( func, initial_x, beta_method = beta_fr, eps=1e-5, maximum_iterations=65536, linesearch=None, *linesearch_args  ):\n",
        "    \"\"\" \n",
        "    Conjugate Gradient\n",
        "    func:               the function to optimize It is called as \"value, gradient = func( x, 1 )\n",
        "    initial_x:          the starting point\n",
        "    eps:                the maximum allowed error in the resulting stepsize t\n",
        "    maximum_iterations: the maximum allowed number of iterations\n",
        "    linesearch:         the linesearch routine\n",
        "    *linesearch_args:   the extra arguments of linesearch routine\n",
        "    \"\"\"\n",
        "\n",
        "    if eps <= 0:\n",
        "        raise ValueError(\"Epsilon must be positive\")\n",
        "    x = np.asarray( initial_x.copy() )\n",
        "\n",
        "    # initialization\n",
        "    values = []\n",
        "    runtimes = []\n",
        "    xs = []\n",
        "    start_time = time.time()\n",
        "    m = len( x )\n",
        "    iterations = 0\n",
        "    direction = np.asmatrix( np.zeros( x.shape ) )\n",
        "\n",
        "    # conjugate gradient updates\n",
        "    while True:\n",
        "        value, gradient = func( x , 1 )\n",
        "        value = np.double( value )\n",
        "        gradient = np.asarray( gradient )\n",
        "        \n",
        "        # updating the logs\n",
        "        values.append( value )\n",
        "        runtimes.append( time.time() - start_time )\n",
        "        xs.append( x.copy() )\n",
        "\n",
        "        # termination criteria\n",
        "        if np.linalg.norm(gradient) < eps:\n",
        "          break\n",
        "        # updating beta and direction\n",
        "        if iterations == 0 or iterations % m == 0:\n",
        "          beta = 0\n",
        "          direction = -1*gradient\n",
        "        else:\n",
        "          beta = beta_method(gradient, old_gradient)\n",
        "          direction = -1*gradient + (beta*direction)\n",
        "\n",
        "        # updating t\n",
        "        if linesearch is None:\n",
        "          t = 1\n",
        "        else:\n",
        "          t = linesearch(func, x, direction, *linesearch_args)\n",
        "        # updating x, gradient and iteration\n",
        "        #x += t * direction\n",
        "        x = x + (t*direction)\n",
        "        old_gradient = gradient\n",
        "        iterations += 1\n",
        "        if iterations >= maximum_iterations:\n",
        "            return (x, values, runtimes, xs)\n",
        "            raise ValueError(\"Too many iterations\")\n",
        "\n",
        "    return (x, values, runtimes, xs)"
      ],
      "metadata": {
        "id": "aGsIGDUfT-Ys"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to evaluate is PSF function:\n",
        "$$ \\min_{-10 \\leq x_i \\leq 10} (x_1+10x_2)^2+5(x_3-x_4)^2+(x_2-2x_3)^4 + 10(x_1-x_4)^4,$$"
      ],
      "metadata": {
        "id": "YGnBnkVWVMrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def psf_func(x, order=0):\n",
        "  x = np.asmatrix(x).T\n",
        "  part1 = (x[0] + 10*x[1])**2\n",
        "  part2 = 5*(x[2]-x[3])**2\n",
        "  part3 = (x[1] - 2*x[2])**4\n",
        "  part4 = 10*(x[0]-x[3])**4\n",
        "  der1 = 2*(20*(x[0]-x[2])**3 + x[0] + 10*x[1])\n",
        "  der2 = 4*(5*(x[0]+10*x[1]) + (x[1]-2*x[2])**3)\n",
        "  der3 = 10*(x[2]-x[3] - 8*(x[1]-2*x[2])**3)\n",
        "  der4 = 10*(-4*(x[0]-x[3])**3 - x[2] + x[3])\n",
        "  value = part1 + part2 + part3 + part4\n",
        "  gradient = [[]]\n",
        "  if order == 0:\n",
        "    return value\n",
        "  elif order == 1:\n",
        "    gradient = np.asmatrix([der1.item(), der2.item(), der3.item(), der4.item()])\n",
        "    return (value, gradient)\n",
        "  else:\n",
        "    raise ValueError(\"The argument \\\"order\\\" should be 0 or 1\")"
      ],
      "metadata": {
        "id": "dRqjdrkYVSzK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing XPOWELL function\n",
        "betas = [beta_fr, beta_pr]\n",
        "for method in betas:\n",
        "    x, values, runtimes, xs = cg(psf_func, [0.5,0.5,0.5,0.5], beta_method=method, linesearch=backtracking_cg)\n",
        "    if method is beta_fr:\n",
        "      print(f'With Fletcher-Reeves gradient update: Iterations= {len(xs)}, solution= {x}')\n",
        "    else:\n",
        "      print(f'With Polak-Riberie gradient update: Iterations= {len(xs)}, solution= {x}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOrlCQTGVVCJ",
        "outputId": "3554781d-d60c-4823-a5d2-8479ac131615"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With Fletcher-Reeves gradient update: Iterations= 9033, solution= [[ 0.00849477 -0.00084948  0.00238687  0.00238791]]\n",
            "With Polak-Riberie gradient update: Iterations= 13132, solution= [[ 0.00848065 -0.00084808  0.00238285  0.00238401]]\n"
          ]
        }
      ]
    }
  ]
}